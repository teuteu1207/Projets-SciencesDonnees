{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20_HES-SO-ARC_646-2.3 SCIENCE DES DONNÉES\n",
    "## House Prices: Advanced Regression Techniques\n",
    "##### Yanick Christe, Mathieu Stalder, Julien Weideli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put simply, CRISP-DM is a comprehensive data mining methodology and process model that provides anyone—from novices to data mining experts—with a complete blueprint for conducting a data mining project. CRISP-DM breaks down the life cycle of a data mining project into six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.\n",
    "\n",
    "**Reference**\n",
    "- Shearer, C. (2000). The CRISP-DM model: the new blueprint for data mining. *Journal of data warehousing*, 5(4), 13-22."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each phase of the process:**\n",
    "1. [Business understanding](#Businessunderstanding)\n",
    "    1. [Determine the Business Objectives](#BusinessObjectives)\n",
    "    2. [Assess the Current Situation](#Assessthecurrentsituation)\n",
    "        1. [Inventory of resources](#Inventory)\n",
    "        2. [Requirements, assumptions and constraints](#Requirements)\n",
    "        3. [Risks and contingencies](#Risks)\n",
    "        4. [Terminology](#Terminology)\n",
    "        5. [Costs and benefits](#CostBenefit)\n",
    "    3. [What are the Desired Outputs](#Desiredoutputs)\n",
    "    4. [What Questions Are We Trying to Answer?](#QA)\n",
    "2. [Data Understanding](#Dataunderstanding)\n",
    "    1. [Initial Data Report](#Datareport)\n",
    "    2. [Describe Data](#Describedata)\n",
    "    3. [Initial Data Exploration](#Exploredata) \n",
    "    4. [Verify Data Quality](#Verifydataquality)\n",
    "        1. [Missing Data](#MissingData) \n",
    "        2. [Outliers](#Outliers) \n",
    "    5. [Data Quality Report](#Dataqualityreport)\n",
    "3. [Data Preparation](#Datapreparation)\n",
    "    1. [Select Your Data](#Selectyourdata)\n",
    "    2. [Cleanse the Data](#Cleansethedata)\n",
    "        1. [Label Encoding](#labelEncoding)\n",
    "        2. [Drop Unnecessary Columns](#DropCols)\n",
    "        3. [Altering Datatypes](#AlteringDatatypes)\n",
    "        4. [Dealing With Zeros](#DealingZeros)\n",
    "    3. [Construct Required Data](#Constructrequireddata)\n",
    "    4. [Integrate Data](#Integratedata)\n",
    "4. [Exploratory Data Analysis](#EDA)\n",
    "5. [Modelling](#Modelling)\n",
    "    1. [Modelling Technique](#ModellingTechnique)\n",
    "    2. [Modelling Assumptions](#ModellingAssumptions)\n",
    "    3. [Build Model](#BuildModel)\n",
    "    4. [Assess Model](#AssessModel)\n",
    "6. [Evaluation](#Evaluation)\n",
    "    1. [Evaluate Results](#EvaluateResults)\n",
    "    2. [Review Process](#ReviewProcess)\n",
    "    3. [Determine Next Steps](#NextSteps)\n",
    "7. [Deployment](#Deployment)\n",
    "    1. [Plan Deployment](#PlanDeployment)\n",
    "    2. [Plan Monitoring and Maintenance](#Monitoring)\n",
    "    3. [Produce Final Report](#FinalReport)\n",
    "    4. [Review Project](#ReviewProject)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Business Understanding  <a class=\"anchor\" id=\"Businessunderstanding\"></a>\n",
    "Perhaps the most important phase of any data mining project, the initial business understanding phase focuses on understanding the project objectives from a business perspective, converting this knowledge into a data mining problem definition, and then developing a preliminary plan designed to achieve the objectives. In order to understand which data should later be analyzed, and how, it is vital for data mining practitioners to fully understand the business for which they are finding a solution.\n",
    "\n",
    "The business understanding phase involves several key steps, including determining business objectives, assessing the situation, determining the data mining goals, and producing the\n",
    "project plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Determine the Business Objectives <a class=\"anchor\" id=\"BusinessObjectives\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding a client’s true goal is critical to uncovering the important factors involved in the planned project—and to ensuring that the project does not result in producing the right answers to the wrong questions. To accomplish this, the data analyst must uncover the primary business objective as well as the related questions the business would like to address.\n",
    "\n",
    "For example, the primary business goal could be to retain current customers by predicting when they are prone to move to a competitor. Examples of related business questions might be, “How does the primary channel (e.g., ATM, branch visit, Internet) of a bank customer affect whether they stay or go?” or “Will lower ATM fees significantly reduce the number of highvalue customers who leave?” A secondary issue might be to determine whether lower fees affect only one particular customer segment.\n",
    "\n",
    "Finally, a good data analyst always determines the measure of success. Success may be measured by reducing lost customers by 10 percent or simply by achieving a better understanding of the customer base. Data analysts should beware of setting unattainable goals and should make sure that each success criterion\n",
    "relates to at least one of the specified business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Assess the Current Situation<a class=\"anchor\" id=\"Assessthecurrentsituation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the data analyst outlines the resources, from personnel to software, that are available to accomplish the data mining project. Particularly important is discovering what data is available to meet the primary business goal. At this point, the data analyst also should list the assumptions made in the project— assumptions such as, “To address the business question, a minimum number of customers over age 50 is necessary.” The data analyst also should list the project risks, list potential solutions to those risks, create a glossary of business and data mining terms,\n",
    "and construct a cost-benefit analysis for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Inventory of resources <a class=\"anchor\" id=\"Inventory\"></a>\n",
    "List the resources available to the project including:\n",
    "- Personnel:\n",
    "- Data: \n",
    "- Computing resources: \n",
    "- Software: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Requirements, assumptions and constraints <a class=\"anchor\" id=\"Requirements\"></a> \n",
    "- Requirements of the project including the schedule of completion\n",
    "- Required comprehensibility and quality of results\n",
    "- Data security concerns as well as any legal issues. \n",
    "- Assumptions made by the project. These may be assumptions about the data that can be verified during data mining, but may also include non-verifiable assumptions about the business related to the project. It is particularly important to list the latter if they will affect the validity of the results. \n",
    "- List the constraints on the project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Risks and contingencies <a class=\"anchor\" id=\"Risks\"></a>\n",
    "- List the risks or events that might delay the project or cause it to fail. \n",
    "- what action will you take if these risks or events take place? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Terminology <a class=\"anchor\" id=\"Terminology\"></a>\n",
    "- A glossary of relevant business terminology\n",
    "- A glossary of data mining terminology, illustrated with examples relevant to the business problem in question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Costs and benefits  <a class=\"anchor\" id=\"CostBenefit\"></a>\n",
    "- Construct a cost-benefit analysis for the project which compares the costs of the project with the potential benefits to the business if it is successful. This comparison should be as specific as possible. For example, you should use financial measures in a commercial situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.3 What are the desired outputs of the project? <a class=\"anchor\" id=\"Desiredoutputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Business success criteria**\n",
    "- \n",
    "- \n",
    "\n",
    "\n",
    "\n",
    "**Data mining success criteria**\n",
    "- \n",
    "- \n",
    "\n",
    "\n",
    "**Produce project plan**\n",
    "- \n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.4 What Questions Are We Trying To Answer? <a class=\"anchor\" id=\"QA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding <a class=\"anchor\" id=\"Dataunderstanding\"></a>\n",
    "The data understanding phase starts with an initial data collection. The analyst then proceeds to increase familiarity with the data, to identify data quality problems, to discover initial insights into the data, or to detect interesting subsets to form hypotheses about hidden information. The data understanding phase involves four steps, including the collection of initial data, the description of data, the exploration of data, and the verification of data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initial Data Report <a class=\"anchor\" id=\"Datareport\"></a>\n",
    "Initial data collection report - \n",
    "List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries Required\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'F:/Projects/Data Science/Defaults/train_/train.csv' does not exist: b'F:/Projects/Data Science/Defaults/train_/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77a78f065fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m'F:/Projects/Data Science/Defaults/train_/train.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# reads the data from the file - denotes as CSV, it has no header, sets column headers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'F:/Projects/Data Science/Defaults/train_/train.csv' does not exist: b'F:/Projects/Data Science/Defaults/train_/train.csv'"
     ]
    }
   ],
   "source": [
    "#Data source: \n",
    "#Source Query location: \n",
    "path =  'F:/Projects/Data Science/Defaults/train_/train.csv'\n",
    "# reads the data from the file - denotes as CSV, it has no header, sets column headers\n",
    "df =  pd.read_csv(path, sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Describe Data <a class=\"anchor\" id=\"Describedata\"></a>\n",
    "Data description report - Describe the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Evaluate whether the data acquired satisfies your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Data Quality <a class=\"anchor\" id=\"Verifydataquality\"></a>\n",
    "\n",
    "Examine the quality of the data, addressing questions such as:\n",
    "\n",
    "- Is the data complete (does it cover all the cases required)?\n",
    "- Is it correct, or does it contain errors and, if there are errors, how common are they?\n",
    "- Are there missing values in the data? If so, how are they represented, where do they occur, and how common are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Missing Data <a class=\"anchor\" id=\"MissingData\"></a>\n",
    "In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let’s get a sense of how many missing values are in each column \n",
    "\n",
    "While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns with > 50% missing\n",
    "missing_df = missing_values_table(df);\n",
    "missing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\n",
    "print('We will remove %d columns.' % len(missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns\n",
    "df = df.drop(list(missing_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Outliers <a class=\"anchor\" id=\"Outliers\"></a>\n",
    "At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the definition of extreme outliers:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Outlier\n",
    "\n",
    "- Below the first quartile − 3 ∗ interquartile range\n",
    "- Above the third quartile + 3 ∗ interquartile range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Initial Data Exploration  <a class=\"anchor\" id=\"Exploredata\"></a>\n",
    "During this stage you'll address data mining questions using querying, data visualization and reporting techniques. These may include:\n",
    "\n",
    "- **Distribution** of key attributes (for example, the target attribute of a prediction task)\n",
    "- **Relationships** between pairs or small numbers of attributes\n",
    "- Results of **simple aggregations**\n",
    "- **Properties** of significant sub-populations\n",
    "- **Simple** statistical analyses\n",
    "\n",
    "These analyses may directly address your data mining goals. They may also contribute to or refine the data description and quality reports, and feed into the transformation and other data preparation steps needed for further analysis. \n",
    "\n",
    "- **Data exploration report** - Describe results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. If appropriate you could include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_table(df):\n",
    "        count_val = df.value_counts()\n",
    "        count_val_percent = 100 * df.value_counts() / len(df)\n",
    "        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n",
    "        count_val_table_ren_columns = count_val_table.rename(\n",
    "        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n",
    "        return count_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "def hist_chart(df, col):\n",
    "        plt.style.use('fivethirtyeight')\n",
    "        plt.hist(df[col].dropna(), edgecolor = 'k');\n",
    "        plt.xlabel(col); plt.ylabel('Number of Entries'); \n",
    "        plt.title('Distribution of '+col);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'account_risk_band'\n",
    "# Histogram & Results\n",
    "hist_chart(df, col)\n",
    "count_values_table(df.account_risk_band)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Correlations  <a class=\"anchor\" id=\"Correlations\"></a>\n",
    "Can we derive any correlation from this data-set. Pairplot chart gives us correlations, distributions and regression path\n",
    "Correlogram are awesome for exploratory analysis. It allows to quickly observe the relationship between every variable of your matrix. \n",
    "It is easy to do it with seaborn: just call the pairplot function\n",
    "\n",
    "Pairplot Documentation cab be found here: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seaborn allows to make a correlogram or correlation matrix really easily. \n",
    "#sns.pairplot(df.dropna().drop(['x'], axis=1), hue='y', kind ='reg')\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_agg = df.drop(['x'], axis=1).groupby(['y']).sum()\n",
    "df_agg = df.groupby(['y']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Differencing\n",
    "Specifically, a new series is constructed where the value at the current time step is calculated as the difference between the original observation and the observation at the previous time step.\n",
    "value(t) = observation(t) - observation(t-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dif_agg = df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differencing\n",
    "#Specifically, a new series is constructed where the value at the current time step is calculated \n",
    "#as the difference between the original observation and the observation at the previous time step.\n",
    "#value(t) = observation(t) - observation(t-1)\n",
    "df_dif = df_dif_agg.diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Data Quality Report <a class=\"anchor\" id=\"Dataqualityreport\"></a>\n",
    "List the results of the data quality verification. If quality problems exist, suggest possible solutions. Solutions to data quality problems generally depend heavily on both data and business knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Préparation des données <a class=\"anchor\" id=\"Datapreparation\"></a>\n",
    "C'est l'étape du projet où vous décidez des données que vous allez utiliser pour l'analyse. Les critères que vous pourriez utiliser pour prendre cette décision doivent comprendre la pertinence des données par rapport à vos objectifs d'exploration de données, la qualité des données, ainsi que les contraintes techniques telles que les limites sur le volume ou les types de données. Notez que la sélection des données couvre la sélection des attributs (colonnes) ainsi que la sélection des enregistrements (lignes) dans un tableau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Sélection des données <a class=\"anchor\" id=\"Selectyourdata\"></a>\n",
    "C'est l'étape du projet où vous décidez des données que vous allez utiliser pour l'analyse. Les critères que vous pourriez utiliser pour prendre cette décision comprennent la pertinence des données par rapport à vos objectifs d'exploration de données, la qualité des données, ainsi que les contraintes techniques telles que les limites sur le volume ou les types de données. Notez que la sélection des données couvre la sélection des attributs (colonnes) ainsi que la sélection des enregistrements (lignes) dans un tableau.\n",
    "\n",
    "Justification de l'inclusion/exclusion - Indiquez les données à inclure/exclure et les raisons de ces décisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_regr = df.drop(['date_maint', 'account_open_date'], axis = 1)\n",
    "X_train = df.drop(['target', 'date_maint', 'account_open_date'], axis = 1)\n",
    "X_test = test.drop(['date_maint', 'account_open_date'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nettoyer les données <a class=\"anchor\" id=\"Cleansethedata\"></a>\n",
    "Cette tâche consiste à élever la qualité des données au niveau requis par les techniques d'analyse que vous avez sélectionnées. Cela peut impliquer la sélection de sous-ensembles de données propres, l'insertion de valeurs par défaut appropriées, ou des techniques plus ambitieuses telles que l'estimation des données manquantes par modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Encodage des étiquettes <a class=\"anchor\" id=\"labelEncoding\"></a>\n",
    "Encodage des étiquettes pour transformer les valeurs catégorielles en nombres entiers\n",
    "\n",
    "Une approche de l'encodage des valeurs catégorielles consiste à utiliser une technique appelée encodage des étiquettes. L'encodage par étiquette consiste simplement à convertir chaque valeur d'une colonne en un nombre. Par exemple, la colonne body_style contient 5 valeurs différentes. Nous pourrions choisir de l'encoder de cette manière :\n",
    "\n",
    "convertible -> 0\n",
    "hardtop -> 1\n",
    "hatchback -> 2\n",
    "berline -> 3\n",
    "wagon -> 4\n",
    "http://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "for col in CAT_COLS:\n",
    "        encoder = LabelEncoder()\n",
    "        X_train[col] = encoder.fit_transform(X_train[col].astype(str))\n",
    "        X_test[col] = encoder.transform(X_test[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column\"] = df[\"column\"].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"column\"] = df[\"column\"].cat.codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Suppression des Colonnes pas nécessaires <a class=\"anchor\" id=\"DropCols\"></a>\n",
    "Parfois, nous n'avons pas besoin de certaines colonnes. Nous pouvons juste garder les données pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_col_list = ['col1', 'col2']\n",
    "\n",
    "df = df.drop(del_col_list, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Modification des types de données <a class=\"anchor\" id=\"AlteringDatatypes\"></a>\n",
    "Parfois, il peut être nécessaire de modifier les types de données. Y compris les types de données d'objets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faire face aux zéros <a class=\"anchor\" id=\"DealingZeros\"></a>\n",
    "Remplacement de tous les zéros des cols. **Note** ajouter / supprimer selon les besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['col1', 'col2']\n",
    "#df[cols] = df[cols].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all the rows with na in the columns mentioned above in the list.\n",
    "\n",
    "# df.dropna(subset=cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 Faire face aux duplications <a class=\"anchor\" id=\"DealingDuplicates\"></a>\n",
    "Supprimer les lignes en double. **Note** : ajouter / supprimer selon les besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Construire les données requises   <a class=\"anchor\" id=\"Constructrequireddata\"></a>\n",
    "Cette tâche comprend des opérations constructives de préparation des données, telles que la production d'attributs dérivés ou de nouveaux enregistrements entiers, ou de valeurs transformées pour des attributs existants.\n",
    "\n",
    "**Attributs dérivés** - Il s'agit de nouveaux attributs qui sont construits à partir d'un ou plusieurs attributs existants dans le même enregistrement. Par exemple, vous pouvez utiliser les variables de longueur et de largeur pour calculer une nouvelle variable de surface.\n",
    "\n",
    "**Enregistrements générés** - Vous décrivez ici la création de tout enregistrement complètement nouveau. Par exemple, vous pouvez avoir besoin de créer des enregistrements pour les clients qui n'ont fait aucun achat au cours de l'année écoulée. Il n'y a aucune raison d'avoir de tels enregistrements dans les données brutes, mais à des fins de modélisation, il peut être utile de représenter explicitement le fait que certains clients n'ont effectué aucun achat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Integration des Données  <a class=\"anchor\" id=\"Integratedata\"></a>\n",
    "Il s'agit de méthodes par lesquelles des informations sont combinées à partir de plusieurs bases de données, tables ou enregistrements pour créer de nouveaux enregistrements ou valeurs.\n",
    "\n",
    "**Données fusionnées** - La fusion de tables fait référence à la réunion de deux ou plusieurs tables qui contiennent des informations différentes sur les mêmes objets. Par exemple, une chaîne de vente au détail peut avoir un tableau contenant des informations sur les caractéristiques générales de chaque magasin (par exemple, la superficie, le type de centre commercial), un autre tableau contenant des données résumées sur les ventes (par exemple, le bénéfice, le pourcentage de variation des ventes par rapport à l'année précédente), et un autre contenant des informations sur la démographie de la région environnante. Chacun de ces tableaux contient un enregistrement pour chaque magasin. Ces tableaux peuvent être fusionnés en un nouveau tableau contenant un enregistrement pour chaque magasin, en combinant les champs des tableaux sources.\n",
    "\n",
    "**Aggrégations** - Les agrégations font référence aux opérations dans lesquelles de nouvelles valeurs sont calculées en résumant les informations de plusieurs enregistrements et/ou tables. Par exemple, la conversion d'une table d'achats de clients où il y a un enregistrement pour chaque achat en une nouvelle table où il y a un enregistrement pour chaque client, avec des champs tels que le nombre d'achats, le montant moyen des achats, le pourcentage de commandes facturées par carte de crédit, le pourcentage d'articles en promotion, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construire notre premier Data Set\n",
    "Join data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis <a class=\"anchor\" id=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset has been prepared, you'll need to analyze it and summarize it's main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling <a class=\"anchor\" id=\"Modelling\"></a>\n",
    "As the first step in modelling, you'll select the actual modelling technique that you'll be using. Although you may have already selected a tool during the business understanding phase, at this stage you'll be selecting the specific modelling technique e.g. decision-tree building with C5.0, or neural network generation with back propagation. If multiple techniques are applied, perform this task separately for each technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Modelling technique <a class=\"anchor\" id=\"ModellingTechnique\"></a>\n",
    "Document the actual modelling technique that is to be used.\n",
    "\n",
    "Import Models below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Modelling assumptions <a class=\"anchor\" id=\"ModellingAssumptions\"></a>\n",
    "Many modelling techniques make specific assumptions about the data, for example that all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any assumptions made.\n",
    "\n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build Model <a class=\"anchor\" id=\"BuildModel\"></a>\n",
    "Run the modelling tool on the prepared dataset to create one or more models.\n",
    "\n",
    "**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n",
    "\n",
    "**Models** - These are the actual models produced by the modelling tool, not a report on the models.\n",
    "\n",
    "**Model descriptions** - Describe the resulting models, report on the interpretation of the models and document any difficulties encountered with their meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Assess Model <a class=\"anchor\" id=\"AssessModel\"></a>\n",
    "Interpret the models according to your domain knowledge, your data mining success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically, then contact business analysts and domain experts later in order to discuss the data mining results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n",
    "\n",
    "At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most data mining projects a single technique is applied more than once and data mining results are generated with several different techniques. \n",
    "\n",
    "**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n",
    "\n",
    "**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model(s). Document all such revisions and assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation <a class=\"anchor\" id=\"Evaluation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to final deployment of the model built by the data analyst, it is important to more thoroughly evaluate the model and review the model’s construction to be certain it properly achieves the business objectives. Here it is critical to determine if some important business issue has not been sufficiently considered. At the end of this phase, the project leader then should decide exactly how to use the data mining results. The key steps here are the evaluation of results, the process review, and the determination of next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Evaluate Results <a class=\"anchor\" id=\"EvaluateResults\"></a>\n",
    "\n",
    "Previous evaluation steps dealt with factors such as the accuracy and generality of the model. This step assesses the degree to which the model meets the business objectives and determines if there is some business reason why this model is deficient. Another option here is to test the model(s) on real-world applications—if time and budget constraints permit. Moreover, evaluation also seeks to unveil additional challenges, information, or hints for future directions.\n",
    "\n",
    "At this stage, the data analyst summarizes the assessment results in terms of business success criteria, including a final statement about whether the project already meets the initial\n",
    "business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Review Process <a class=\"anchor\" id=\"ReviewProcess\"></a>\n",
    "\n",
    "It is now appropriate to do a more thorough review of the data mining engagement to determine if there is any important factor or task that has somehow been overlooked. This review also covers quality assurance issues (e.g., did we correctly build the model? Did we only use allowable attributes that are available for future deployment?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Determine Next Steps <a class=\"anchor\" id=\"NextSteps\"></a>\n",
    "\n",
    "At this stage, the project leader must decide whether to finish this project and move on to deployment or whether to initiate further iterations or set up new data mining projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deployment  <a class=\"anchor\" id=\"Deployment\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation is generally not the end of the project. The knowledge gained must be organized and presented in a way that the customer can use it, which often involves applying “live” models within an organization’s decision-making processes, such as the real-time personalization of Web pages or repeated scoring of marketing databases.\n",
    "\n",
    "Depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data mining process across the enterprise. Even though it is often the customer, not the data analyst, who carries out the deployment steps, it is important for the customer to understand up front what actions must be taken in order to actually make use of the created models. The key steps here are plan deployment, plan monitoring and maintenance, the production of the final report, and review of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Plan Deployment <a class=\"anchor\" id=\"PlanDeployment\"></a>\n",
    "\n",
    "In order to deploy the data mining result(s) into the business, this task takes the evaluation results and develops a strategy for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Plan Monitoring and Maintenance <a class=\"anchor\" id=\"Monitoring\"></a>\n",
    "Monitoring and maintenance are important issues if the data mining result is to become part of the day-to-day business and its environment. A carefully prepared maintenance strategy avoids incorrect usage of data mining results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Produce Final Report <a class=\"anchor\" id=\"FinalReport\"></a>\n",
    "At the end of the project, the project leader and his or her team write up a final report. Depending on the deployment plan, this report may be only a summary of the project and its experiences (if they have not already been documented as an ongoing activity) or it may be a final and comprehensive presentation of the data mining result(s). This report includes all of the previous deliverables and summarizes and organizes the results. Also, there often will be a meeting at the conclusion of the project, where the results are verbally presented to the customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Review Project <a class=\"anchor\" id=\"ReviewProject\"></a>\n",
    "The data analyst should assess failures and successes as well as potential areas of improvement for use in future projects. This step should include a summary of important experiences during the project and can include interviews with the significant project participants. This document could include pitfalls, misleading approaches, or hints for selecting the best-suited data mining techniques in similar situations. In ideal projects, experience documentation also covers any reports written by individual project members during the project phases and tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
